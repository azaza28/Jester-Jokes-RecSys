{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0759cbe0-f5ae-4af0-8d63-f6ff47e1859c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting everything up for the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39204990-f835-4ad9-8e57-df7983f788fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532bddb3-8b36-4f02-ad69-cbb0c84eb690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:22.908223Z",
     "iopub.status.busy": "2024-12-08T08:33:22.907620Z",
     "iopub.status.idle": "2024-12-08T08:33:46.504644Z",
     "shell.execute_reply": "2024-12-08T08:33:46.503406Z",
     "shell.execute_reply.started": "2024-12-08T08:33:22.908200Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-12-08 08:33:32.326354: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-08 08:33:33.724332: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-08 08:33:37.875510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356cbb2-8c42-4063-84ef-881e83276654",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Definition of needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1313c3-356f-4a7f-8b79-ab788e8d09d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:46.506583Z",
     "iopub.status.busy": "2024-12-08T08:33:46.505715Z",
     "iopub.status.idle": "2024-12-08T08:33:46.516658Z",
     "shell.execute_reply": "2024-12-08T08:33:46.516142Z",
     "shell.execute_reply.started": "2024-12-08T08:33:46.506558Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_embedding(text: str):\n",
    "    \"\"\"\n",
    "    Generates an embedding (vector representation) for a given text.\n",
    "\n",
    "    This function takes a text input, tokenizes it using a tokenizer,\n",
    "    processes the tokens through a pre-trained model, and returns the \n",
    "    mean of the last hidden state as a 1-dimensional list of floats.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text for which the embedding will be generated.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list[float]\n",
    "        A 1-dimensional list representing the embedding of the input text.\n",
    "\n",
    "    Example Usage:\n",
    "    --------------\n",
    "    embedding = generate_embedding(\"Example text for generating an embedding.\")\n",
    "    print(embedding)  # Outputs a list of floats\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f7673-2a78-46b6-ab8b-0740237a3e06",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d02225-09a5-425f-a720-af50ade12f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:46.517756Z",
     "iopub.status.busy": "2024-12-08T08:33:46.517359Z",
     "iopub.status.idle": "2024-12-08T08:33:46.531463Z",
     "shell.execute_reply": "2024-12-08T08:33:46.530975Z",
     "shell.execute_reply.started": "2024-12-08T08:33:46.517738Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6d41da32b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61616066-ae46-4add-be87-faeb27bcbc92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93be20ef-7039-48fa-8311-2318f8557163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:46.533155Z",
     "iopub.status.busy": "2024-12-08T08:33:46.532603Z",
     "iopub.status.idle": "2024-12-08T08:33:46.717987Z",
     "shell.execute_reply": "2024-12-08T08:33:46.717376Z",
     "shell.execute_reply.started": "2024-12-08T08:33:46.533135Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pl.read_parquet(\"../data/processed/train_data.parquet\")  # userId, jokeId, rating (TRAIN ONLY)\n",
    "test_data = pl.read_parquet(\"../data/processed/test_data.parquet\")    # userId, jokeId, rating (TEST ONLY)\n",
    "items = pl.read_parquet(\"../data/processed/shuffled_jokes.parquet\")    # jokeId, jokeText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cfabd-870c-4b18-ab19-5da14d5ffa79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Generate Joke-Level Text Features (Independent of Train/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dcbdc16-538e-4f9b-b824-7865831f5849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:46.719296Z",
     "iopub.status.busy": "2024-12-08T08:33:46.718693Z",
     "iopub.status.idle": "2024-12-08T08:33:46.811567Z",
     "shell.execute_reply": "2024-12-08T08:33:46.811023Z",
     "shell.execute_reply.started": "2024-12-08T08:33:46.719273Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = items.with_columns([\n",
    "    pl.col(\"jokeText\").str.lengths().alias(\"text_length\"),\n",
    "    pl.col(\"jokeText\").str.split(\" \").arr.lengths().alias(\"word_count\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b26c35-ce5d-4f5b-909e-605b9170c4a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a280b2-63ba-4d82-b7ae-10eacc63287a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:46.812991Z",
     "iopub.status.busy": "2024-12-08T08:33:46.812258Z",
     "iopub.status.idle": "2024-12-08T08:33:46.826554Z",
     "shell.execute_reply": "2024-12-08T08:33:46.826043Z",
     "shell.execute_reply.started": "2024-12-08T08:33:46.812958Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\", device=0)\n",
    "# items = items.with_columns(\n",
    "#     pl.col(\"jokeText\").apply(lambda txt: sentiment_analyzer(txt)[0][\"score\"]).alias(\"sentiment_score\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6db9c1-7640-4228-a4b3-1d126ed9db4f",
   "metadata": {},
   "source": [
    "> Showed no effect in my case with jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f29fa-c88b-4442-83ee-d34bcc4e06d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Compute Embeddings for Each Joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be889643-13c8-4a19-a168-597dac346e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:46.827892Z",
     "iopub.status.busy": "2024-12-08T08:33:46.827248Z",
     "iopub.status.idle": "2024-12-08T08:33:50.671577Z",
     "shell.execute_reply": "2024-12-08T08:33:50.670834Z",
     "shell.execute_reply.started": "2024-12-08T08:33:46.827860Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = [generate_embedding(txt) for txt in items[\"jokeText\"]]\n",
    "items = items.with_columns(pl.Series(name=\"embeddings\", values=embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c04f3-d4da-44bf-9f04-1b740c5afbd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Compute Rating-Based Features From Train Only (No Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7516bdf-1272-4e1e-9e99-9c9b0b67c935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.673222Z",
     "iopub.status.busy": "2024-12-08T08:33:50.672528Z",
     "iopub.status.idle": "2024-12-08T08:33:50.738282Z",
     "shell.execute_reply": "2024-12-08T08:33:50.737483Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.673196Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "joke_stats = (\n",
    "    train_data.groupby(\"jokeId\")\n",
    "    .agg([\n",
    "        pl.count(\"rating\").alias(\"num_ratings\"),\n",
    "        pl.col(\"rating\").mean().alias(\"avg_rating\"),\n",
    "        pl.col(\"rating\").std().alias(\"rating_std\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "user_stats = (\n",
    "    train_data.groupby(\"userId\")\n",
    "    .agg([\n",
    "        pl.count(\"rating\").alias(\"num_ratings_user\"),\n",
    "        pl.col(\"rating\").mean().alias(\"avg_user_rating\"),\n",
    "        pl.col(\"rating\").std().alias(\"user_rating_std_dev\"),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e14426-c150-4bc2-8fce-a0873333b18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.740199Z",
     "iopub.status.busy": "2024-12-08T08:33:50.739471Z",
     "iopub.status.idle": "2024-12-08T08:33:50.765353Z",
     "shell.execute_reply": "2024-12-08T08:33:50.764804Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.740176Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>jokeId</th><th>jokeText</th><th>text_length</th><th>word_count</th><th>embeddings</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 5)\n",
       "┌────────┬──────────┬─────────────┬────────────┬────────────┐\n",
       "│ jokeId ┆ jokeText ┆ text_length ┆ word_count ┆ embeddings │\n",
       "│ ---    ┆ ---      ┆ ---         ┆ ---        ┆ ---        │\n",
       "│ u32    ┆ u32      ┆ u32         ┆ u32        ┆ u32        │\n",
       "╞════════╪══════════╪═════════════╪════════════╪════════════╡\n",
       "│ 0      ┆ 0        ┆ 0           ┆ 0          ┆ 0          │\n",
       "└────────┴──────────┴─────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a96f37-0291-4508-a7e4-d1b9b9ac3265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.767431Z",
     "iopub.status.busy": "2024-12-08T08:33:50.766968Z",
     "iopub.status.idle": "2024-12-08T08:33:50.848393Z",
     "shell.execute_reply": "2024-12-08T08:33:50.847757Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.767412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute global means from train_data for filling missing values in test_data\n",
    "# These are only from train_data\n",
    "global_means = {\n",
    "    \"num_ratings\": float(joke_stats[\"num_ratings\"].mean()) if joke_stats.height > 0 else 0.0,\n",
    "    \"avg_rating\": float(train_data[\"rating\"].mean()) if train_data.height > 0 else 5.0,  # fallback mean rating\n",
    "    \"rating_std\": float(train_data[\"rating\"].std()) if train_data.height > 1 else 0.0,\n",
    "    \"num_ratings_user\": float(user_stats[\"num_ratings_user\"].mean()) if user_stats.height > 0 else 1.0,\n",
    "    \"avg_user_rating\": float(train_data[\"rating\"].mean()) if train_data.height > 0 else 5.0,\n",
    "    \"user_rating_std_dev\": float(train_data[\"rating\"].std()) if train_data.height > 1 else 0.0,\n",
    "}\n",
    "\n",
    "# Join these rating-based stats into items\n",
    "items = items.join(joke_stats, on=\"jokeId\", how=\"left\")\n",
    "\n",
    "# Fill missing joke stats in items with global means\n",
    "# Even though we do can skip it in our case, it is better to leave this POC\n",
    "items = items.with_columns([\n",
    "    pl.col(\"num_ratings\").fill_null(global_means[\"num_ratings\"]),\n",
    "    pl.col(\"avg_rating\").fill_null(global_means[\"avg_rating\"]),\n",
    "    pl.col(\"rating_std\").fill_null(global_means[\"rating_std\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f7a9d-7e1c-4c01-9e80-358c1702cf6d",
   "metadata": {},
   "source": [
    "> At this point, items has embeddings and full joke-level features derived from train_data only.\n",
    "We'll save this as items_with_all_features.parquet for future clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c66927-8f6e-4361-b81f-373acd600fe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.849696Z",
     "iopub.status.busy": "2024-12-08T08:33:50.849134Z",
     "iopub.status.idle": "2024-12-08T08:33:50.898991Z",
     "shell.execute_reply": "2024-12-08T08:33:50.898405Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.849677Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "items.write_parquet(\"../data/processed/items_with_all_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f223b1e-de03-49fa-a8ab-ccf8003439f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Prepare Train and Test Feature Datasets Without Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236477c6-4f6c-4cfa-90b3-8c7a335f6948",
   "metadata": {},
   "source": [
    "> Since embeddings can be large and not needed directly in train/test sets, we exclude embeddings column from train/test output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b188b3e9-b2d4-4c14-9910-e77ac6c93e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.900268Z",
     "iopub.status.busy": "2024-12-08T08:33:50.899678Z",
     "iopub.status.idle": "2024-12-08T08:33:50.938347Z",
     "shell.execute_reply": "2024-12-08T08:33:50.937611Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.900248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll join items (excluding embeddings) + user_stats and joke_stats to train/test data.\n",
    "\n",
    "# For train_data: join user_stats and joke_stats from train_data only\n",
    "train_data = train_data.join(items.select([\n",
    "    \"jokeId\", \"text_length\", \"word_count\",\n",
    "    \"num_ratings\", \"avg_rating\", \"rating_std\"\n",
    "]), on=\"jokeId\", how=\"left\")\n",
    "\n",
    "# Add user-level features to train_data\n",
    "train_data = train_data.join(user_stats, on=\"userId\", how=\"left\")\n",
    "\n",
    "# Fill missing in train_data\n",
    "train_data = train_data.with_columns([\n",
    "    pl.col(\"num_ratings_user\").fill_null(global_means[\"num_ratings_user\"]),\n",
    "    pl.col(\"avg_user_rating\").fill_null(global_means[\"avg_user_rating\"]),\n",
    "    pl.col(\"user_rating_std_dev\").fill_null(global_means[\"user_rating_std_dev\"])\n",
    "])\n",
    "\n",
    "# For test_data: join items (excluding embeddings)\n",
    "test_data = test_data.join(items.select([\n",
    "    \"jokeId\", \"text_length\", \"word_count\",\n",
    "    \"num_ratings\", \"avg_rating\", \"rating_std\"\n",
    "]), on=\"jokeId\", how=\"left\")\n",
    "\n",
    "# Join user_stats to test_data\n",
    "test_data = test_data.join(user_stats, on=\"userId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066acdb1-31b0-4da0-9352-8e1c65012944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.939976Z",
     "iopub.status.busy": "2024-12-08T08:33:50.939216Z",
     "iopub.status.idle": "2024-12-08T08:33:50.948463Z",
     "shell.execute_reply": "2024-12-08T08:33:50.947931Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.939952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>userId</th><th>jokeId</th><th>rating</th><th>text_length</th><th>word_count</th><th>num_ratings</th><th>avg_rating</th><th>rating_std</th><th>num_ratings_user</th><th>avg_user_rating</th><th>user_rating_std_dev</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 11)\n",
       "┌────────┬────────┬────────┬───────────┬───┬────────────┬────────────┬──────────────┬──────────────┐\n",
       "│ userId ┆ jokeId ┆ rating ┆ text_leng ┆ … ┆ rating_std ┆ num_rating ┆ avg_user_rat ┆ user_rating_ │\n",
       "│ ---    ┆ ---    ┆ ---    ┆ th        ┆   ┆ ---        ┆ s_user     ┆ ing          ┆ std_dev      │\n",
       "│ u32    ┆ u32    ┆ u32    ┆ ---       ┆   ┆ u32        ┆ ---        ┆ ---          ┆ ---          │\n",
       "│        ┆        ┆        ┆ u32       ┆   ┆            ┆ u32        ┆ u32          ┆ u32          │\n",
       "╞════════╪════════╪════════╪═══════════╪═══╪════════════╪════════════╪══════════════╪══════════════╡\n",
       "│ 0      ┆ 0      ┆ 0      ┆ 0         ┆ … ┆ 0          ┆ 0          ┆ 0            ┆ 0            │\n",
       "└────────┴────────┴────────┴───────────┴───┴────────────┴────────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718f252a-5b87-4b46-adbd-89dab6a3c72c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.949606Z",
     "iopub.status.busy": "2024-12-08T08:33:50.949107Z",
     "iopub.status.idle": "2024-12-08T08:33:50.957260Z",
     "shell.execute_reply": "2024-12-08T08:33:50.956733Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.949586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill missing in test_data using global means\n",
    "# The same as before, we will leave it,\n",
    "# imagining, that it might be needed in the future\n",
    "test_data = test_data.with_columns([\n",
    "    pl.col(\"num_ratings_user\").fill_null(global_means[\"num_ratings_user\"]),\n",
    "    pl.col(\"avg_user_rating\").fill_null(global_means[\"avg_user_rating\"]),\n",
    "    pl.col(\"user_rating_std_dev\").fill_null(global_means[\"user_rating_std_dev\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e8faf-213e-4793-b159-a83376bb3f12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Save Final Train and Test Features Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284f5521-ef6d-4e6c-8164-8537183e1927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:33:50.958440Z",
     "iopub.status.busy": "2024-12-08T08:33:50.957890Z",
     "iopub.status.idle": "2024-12-08T08:33:51.158329Z",
     "shell.execute_reply": "2024-12-08T08:33:51.157610Z",
     "shell.execute_reply.started": "2024-12-08T08:33:50.958422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.with_columns(\n",
    "    (pl.col(\"rating\") >= 0).cast(pl.Int8).alias(\"rating\")\n",
    ")\n",
    "\n",
    "test_data = test_data.with_columns(\n",
    "    (pl.col(\"rating\") >= 0).cast(pl.Int8).alias(\"rating\")\n",
    ")\n",
    "\n",
    "\n",
    "train_data.write_parquet(\"../data/processed/train_features.parquet\")\n",
    "test_data.write_parquet(\"../data/processed/test_features.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
